{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "toc-autonumbering": true,
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mrbestnaija/machine-learning-engineering-for-production-public/blob/main/Bestman_Solution_to_Feature_Engineering_Pipeline_Bike_Sharing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23R0Z9RojXYW"
      },
      "source": [
        "# Feature Engineering Pipeline : public bicycles rented per hour in the Seoul Bike Sharing System\n",
        "\n",
        "\n",
        "This Project creates a pipeline for developing a Model that makes prediction of bike count required at each hour for the stable supply of rental bikes.\n",
        "\n",
        "The dataset contains count of public bicycles rented per hour in the Seoul Bike Sharing System, with corresponding weather data and holiday information.\n",
        "\n",
        "#I must acknowledge that this guideline solution is the original work of Fabio\n",
        "\n",
        "The Machine Learning project pipeline will build end-to-end pipelines in future courses but for this one, you will only build up to the feature engineering part. Specifically, you will:\n",
        "\n",
        "* ingest data from a base directory with `ExampleGen`\n",
        "* compute the statistics of the training data with `StatisticsGen`\n",
        "* infer a schema with `SchemaGen`\n",
        "* detect anomalies in the evaluation data with `ExampleValidator`\n",
        "* preprocess the data into features suitable for model training with `Transform`\n",
        "\n",
        "If several steps mentioned above sound familiar, it's because the TFX components that deal with data validation and analysis (i.e. `StatisticsGen`, `SchemaGen`, `ExampleValidator`) uses [Tensorflow Data Validation (TFDV)](https://www.tensorflow.org/tfx/data_validation/get_started) under the hood. You're already familiar with this library from the exercises in Week 1 and for this week, you'll see how it fits within an ML pipeline.\n",
        "\n",
        "The components you will use are the orange boxes highlighted in the figure below:\n",
        "\n",
        "\n",
        "\n",
        "<img src='img/feature_eng_pipeline.png'>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GivNBNYjb3b"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-ePgV0Lj68Q"
      },
      "source": [
        "### Import packages\n",
        "\n",
        "Let's begin by importing the required packages and modules. In case you want to replicate this in your local workstation, we used *Tensorflow v2.5.1* and *TFX v1.2.0*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIqpWK9efviJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cddb5a81-20ef-459e-bce5-568625397557"
      },
      "source": [
        "import os\n",
        "if 'google.colab' in str(get_ipython()):\n",
        "    colab=\"yes\"\n",
        "    print('Running on CoLab')\n",
        "    try:\n",
        "      import tfx\n",
        "    except ModuleNotFoundError:\n",
        "      !pip install tfx\n",
        "else:\n",
        "    colab=\"no\"\n",
        "    print('Not running on CoLab')\n",
        "if colab == \"no\":\n",
        "    try:\n",
        "      import tensorflow as tf\n",
        "    except ModuleNotFoundError:\n",
        "      !pip install tensorflow==2.5.0\n",
        "    try:\n",
        "      import tfx\n",
        "    except ModuleNotFoundError:\n",
        "      !pip install tfx"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on CoLab\n",
            "Collecting tfx\n",
            "  Downloading tfx-1.14.0-py3-none-any.whl (2.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ml-pipelines-sdk==1.14.0 (from tfx)\n",
            "  Downloading ml_pipelines_sdk-1.14.0-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m80.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py<2.0.0,>=0.9 in /usr/local/lib/python3.10/dist-packages (from tfx) (1.4.0)\n",
            "Collecting ml-metadata<1.15.0,>=1.14.0 (from tfx)\n",
            "  Downloading ml_metadata-1.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m77.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting packaging<21,>=20 (from tfx)\n",
            "  Downloading packaging-20.9-py2.py3-none-any.whl (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: portpicker<2,>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from tfx) (1.5.2)\n",
            "Requirement already satisfied: protobuf<5,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tfx) (3.20.3)\n",
            "Collecting docker<5,>=4.1 (from tfx)\n",
            "  Downloading docker-4.4.4-py2.py3-none-any.whl (147 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.0/147.0 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-apitools<1,>=0.5 (from tfx)\n",
            "  Downloading google_apitools-0.5.32-py3-none-any.whl (135 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.7/135.7 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-api-python-client<2,>=1.8 (from tfx)\n",
            "  Downloading google_api_python_client-1.12.11-py2.py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jinja2<4,>=2.7.3 in /usr/local/lib/python3.10/dist-packages (from tfx) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=3.10.0.2 in /usr/local/lib/python3.10/dist-packages (from tfx) (4.5.0)\n",
            "Collecting apache-beam[gcp]<3,>=2.47 (from tfx)\n",
            "  Downloading apache_beam-2.51.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.7/14.7 MB\u001b[0m \u001b[31m79.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting attrs<22,>=19.3.0 (from tfx)\n",
            "  Downloading attrs-21.4.0-py2.py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click<9,>=7 in /usr/local/lib/python3.10/dist-packages (from tfx) (8.1.7)\n",
            "Requirement already satisfied: google-api-core<3 in /usr/local/lib/python3.10/dist-packages (from tfx) (2.11.1)\n",
            "Collecting google-cloud-aiplatform<2,>=1.6.2 (from tfx)\n",
            "  Downloading google_cloud_aiplatform-1.36.1-py2.py3-none-any.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m67.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-cloud-bigquery<3,>=2.26.0 (from tfx)\n",
            "  Downloading google_cloud_bigquery-2.34.4-py2.py3-none-any.whl (206 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m206.6/206.6 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: grpcio<2,>=1.28.1 in /usr/local/lib/python3.10/dist-packages (from tfx) (1.59.2)\n",
            "Collecting keras-tuner<2,>=1.0.4 (from tfx)\n",
            "  Downloading keras_tuner-1.4.6-py3-none-any.whl (128 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.9/128.9 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting kubernetes<13,>=10.0.1 (from tfx)\n",
            "  Downloading kubernetes-12.0.1-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m89.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1.16 in /usr/local/lib/python3.10/dist-packages (from tfx) (1.23.5)\n",
            "Collecting pyarrow<11,>=10 (from tfx)\n",
            "  Downloading pyarrow-10.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.9/35.9 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml<7,>=6 in /usr/local/lib/python3.10/dist-packages (from tfx) (6.0.1)\n",
            "Collecting tensorflow<2.14,>=2.13.0 (from tfx)\n",
            "  Downloading tensorflow-2.13.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (479.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m479.7/479.7 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-hub<0.14,>=0.9.0 (from tfx)\n",
            "  Downloading tensorflow_hub-0.13.0-py2.py3-none-any.whl (100 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.6/100.6 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-data-validation<1.15.0,>=1.14.0 (from tfx)\n",
            "  Downloading tensorflow_data_validation-1.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.1/19.1 MB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-model-analysis<0.46.0,>=0.45.0 (from tfx)\n",
            "  Downloading tensorflow_model_analysis-0.45.0-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-serving-api!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<3,>=1.15 (from tfx)\n",
            "  Downloading tensorflow_serving_api-2.14.0-py2.py3-none-any.whl (26 kB)\n",
            "Collecting tensorflow-transform<1.15.0,>=1.14.0 (from tfx)\n",
            "  Downloading tensorflow_transform-1.14.0-py3-none-any.whl (447 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m447.8/447.8 kB\u001b[0m \u001b[31m46.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tfx-bsl<1.15.0,>=1.14.0 (from tfx)\n",
            "  Downloading tfx_bsl-1.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (22.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.5/22.5 MB\u001b[0m \u001b[31m62.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting crcmod<2.0,>=1.7 (from apache-beam[gcp]<3,>=2.47->tfx)\n",
            "  Downloading crcmod-1.7.tar.gz (89 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.7/89.7 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting orjson<4,>=3.9.7 (from apache-beam[gcp]<3,>=2.47->tfx)\n",
            "  Downloading orjson-3.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dill<0.3.2,>=0.3.1.1 (from apache-beam[gcp]<3,>=2.47->tfx)\n",
            "  Downloading dill-0.3.1.1.tar.gz (151 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.0/152.0 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cloudpickle~=2.2.1 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (2.2.1)\n",
            "Collecting fastavro<2,>=0.23.6 (from apache-beam[gcp]<3,>=2.47->tfx)\n",
            "  Downloading fastavro-1.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m99.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fasteners<1.0,>=0.3 (from apache-beam[gcp]<3,>=2.47->tfx)\n",
            "  Downloading fasteners-0.19-py3-none-any.whl (18 kB)\n",
            "Collecting hdfs<3.0.0,>=2.1.0 (from apache-beam[gcp]<3,>=2.47->tfx)\n",
            "  Downloading hdfs-2.7.3.tar.gz (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: httplib2<0.23.0,>=0.8 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (0.22.0)\n",
            "Collecting js2py<1,>=0.74 (from apache-beam[gcp]<3,>=2.47->tfx)\n",
            "  Downloading Js2Py-0.74-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m80.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting objsize<0.7.0,>=0.6.1 (from apache-beam[gcp]<3,>=2.47->tfx)\n",
            "  Downloading objsize-0.6.1-py3-none-any.whl (9.3 kB)\n",
            "INFO: pip is looking at multiple versions of apache-beam[gcp] to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting apache-beam[gcp]<3,>=2.47 (from tfx)\n",
            "  Downloading apache_beam-2.50.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.7/14.7 MB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pymongo<5.0.0,>=3.8.0 (from apache-beam[gcp]<3,>=2.47->tfx)\n",
            "  Downloading pymongo-4.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (677 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m677.1/677.1 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: proto-plus<2,>=1.7.1 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (1.22.3)\n",
            "Requirement already satisfied: pydot<2,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (1.4.2)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2018.3 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (2023.3.post1)\n",
            "Requirement already satisfied: regex>=2020.6.8 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (2023.6.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.24.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (2.31.0)\n",
            "Collecting zstandard<1,>=0.18.0 (from apache-beam[gcp]<3,>=2.47->tfx)\n",
            "  Downloading zstandard-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m95.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cachetools<6,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (5.3.2)\n",
            "Collecting google-apitools<1,>=0.5 (from tfx)\n",
            "  Downloading google-apitools-0.5.31.tar.gz (173 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.5/173.5 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: google-auth<3,>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (2.17.3)\n",
            "Requirement already satisfied: google-auth-httplib2<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (0.1.1)\n",
            "Requirement already satisfied: google-cloud-datastore<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (2.15.2)\n",
            "Collecting google-cloud-pubsub<3,>=2.1.0 (from apache-beam[gcp]<3,>=2.47->tfx)\n",
            "  Downloading google_cloud_pubsub-2.18.4-py2.py3-none-any.whl (265 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.9/265.9 kB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-cloud-pubsublite<2,>=1.2.0 (from apache-beam[gcp]<3,>=2.47->tfx)\n",
            "  Downloading google_cloud_pubsublite-1.8.3-py2.py3-none-any.whl (288 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.1/288.1 kB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: google-cloud-bigquery-storage<3,>=2.6.3 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (2.22.0)\n",
            "Requirement already satisfied: google-cloud-core<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (2.3.3)\n",
            "Collecting google-cloud-bigtable<3,>=2.19.0 (from apache-beam[gcp]<3,>=2.47->tfx)\n",
            "  Downloading google_cloud_bigtable-2.21.0-py2.py3-none-any.whl (293 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.0/293.0 kB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-cloud-spanner<4,>=3.0.0 (from apache-beam[gcp]<3,>=2.47->tfx)\n",
            "  Downloading google_cloud_spanner-3.40.1-py2.py3-none-any.whl (332 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m332.9/332.9 kB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-cloud-dlp<4,>=3.0.0 (from apache-beam[gcp]<3,>=2.47->tfx)\n",
            "  Downloading google_cloud_dlp-3.13.0-py2.py3-none-any.whl (158 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.3/158.3 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: google-cloud-language<3,>=2.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (2.9.1)\n",
            "Collecting google-cloud-videointelligence<3,>=2.0 (from apache-beam[gcp]<3,>=2.47->tfx)\n",
            "  Downloading google_cloud_videointelligence-2.11.4-py2.py3-none-any.whl (229 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.5/229.5 kB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-cloud-vision<4,>=2 (from apache-beam[gcp]<3,>=2.47->tfx)\n",
            "  Downloading google_cloud_vision-3.4.5-py2.py3-none-any.whl (444 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m444.1/444.1 kB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-cloud-recommendations-ai<0.11.0,>=0.1.0 (from apache-beam[gcp]<3,>=2.47->tfx)\n",
            "  Downloading google_cloud_recommendations_ai-0.10.5-py2.py3-none-any.whl (173 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.4/173.4 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker<5,>=4.1->tfx) (1.16.0)\n",
            "Requirement already satisfied: websocket-client>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from docker<5,>=4.1->tfx) (1.6.4)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core<3->tfx) (1.61.0)\n",
            "Collecting uritemplate<4dev,>=3.0.0 (from google-api-python-client<2,>=1.8->tfx)\n",
            "  Downloading uritemplate-3.0.1-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: oauth2client>=1.4.12 in /usr/local/lib/python3.10/dist-packages (from google-apitools<1,>=0.5->tfx) (4.1.3)\n",
            "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2,>=1.6.2->tfx) (2.8.0)\n",
            "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2,>=1.6.2->tfx) (1.10.4)\n",
            "Requirement already satisfied: shapely<3.0.0dev in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2,>=1.6.2->tfx) (2.0.2)\n",
            "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery<3,>=2.26.0->tfx) (2.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2<4,>=2.7.3->tfx) (2.1.3)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (from keras-tuner<2,>=1.0.4->tfx) (2.14.0)\n",
            "Collecting kt-legacy (from keras-tuner<2,>=1.0.4->tfx)\n",
            "  Downloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.10/dist-packages (from kubernetes<13,>=10.0.1->tfx) (2023.7.22)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes<13,>=10.0.1->tfx) (67.7.2)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes<13,>=10.0.1->tfx) (1.3.1)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes<13,>=10.0.1->tfx) (2.0.7)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from packaging<21,>=20->tfx) (3.1.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from portpicker<2,>=1.3.1->tfx) (5.9.5)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tfx) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.1.21 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tfx) (23.5.26)\n",
            "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow<2.14,>=2.13.0->tfx)\n",
            "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tfx) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tfx) (3.9.0)\n",
            "Collecting keras (from keras-tuner<2,>=1.0.4->tfx)\n",
            "  Downloading keras-2.13.1-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m96.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tfx) (16.0.6)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tfx) (3.3.0)\n",
            "Collecting tensorboard<2.14,>=2.13 (from tensorflow<2.14,>=2.13.0->tfx)\n",
            "  Downloading tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m111.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-estimator<2.14,>=2.13.0 (from tensorflow<2.14,>=2.13.0->tfx)\n",
            "  Downloading tensorflow_estimator-2.13.0-py2.py3-none-any.whl (440 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.8/440.8 kB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tfx) (2.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tfx) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tfx) (0.34.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-data-validation<1.15.0,>=1.14.0->tfx) (1.3.2)\n",
            "Requirement already satisfied: pandas<2,>=1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-data-validation<1.15.0,>=1.14.0->tfx) (1.5.3)\n",
            "Collecting pyfarmhash<0.4,>=0.2.2 (from tensorflow-data-validation<1.15.0,>=1.14.0->tfx)\n",
            "  Downloading pyfarmhash-0.3.2.tar.gz (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.9/99.9 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tensorflow-metadata<1.15,>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-data-validation<1.15.0,>=1.14.0->tfx) (1.14.0)\n",
            "Requirement already satisfied: ipython<8,>=7 in /usr/local/lib/python3.10/dist-packages (from tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (7.34.0)\n",
            "Requirement already satisfied: ipywidgets<8,>=7 in /usr/local/lib/python3.10/dist-packages (from tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (7.7.1)\n",
            "Requirement already satisfied: pillow>=9.4.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (9.4.0)\n",
            "Requirement already satisfied: scipy<2,>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (1.11.3)\n",
            "INFO: pip is looking at multiple versions of tensorflow-serving-api to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting tensorflow-serving-api!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<3,>=1.15 (from tfx)\n",
            "  Downloading tensorflow_serving_api-2.13.1-py2.py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.14,>=2.13.0->tfx) (0.41.3)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core<3->tfx) (1.48.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.18.0->apache-beam[gcp]<3,>=2.47->tfx) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.18.0->apache-beam[gcp]<3,>=2.47->tfx) (4.9)\n",
            "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigtable<3,>=2.19.0->apache-beam[gcp]<3,>=2.47->tfx) (0.12.6)\n",
            "Collecting overrides<7.0.0,>=6.0.1 (from google-cloud-pubsublite<2,>=1.2.0->apache-beam[gcp]<3,>=2.47->tfx)\n",
            "  Downloading overrides-6.5.0-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: sqlparse>=0.4.4 in /usr/local/lib/python3.10/dist-packages (from google-cloud-spanner<4,>=3.0.0->apache-beam[gcp]<3,>=2.47->tfx) (0.4.4)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<3,>=2.26.0->tfx) (1.5.0)\n",
            "Collecting docopt (from hdfs<3.0.0,>=2.1.0->apache-beam[gcp]<3,>=2.47->tfx)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jedi>=0.16 (from ipython<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx)\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m71.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (3.0.39)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (4.8.0)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (5.5.6)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (0.2.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (3.6.6)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (3.0.9)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.10/dist-packages (from oauth2client>=1.4.12->google-apitools<1,>=0.5->tfx) (0.5.0)\n",
            "Collecting dnspython<3.0.0,>=1.16.0 (from pymongo<5.0.0,>=3.8.0->apache-beam[gcp]<3,>=2.47->tfx)\n",
            "  Downloading dnspython-2.4.2-py3-none-any.whl (300 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.4/300.4 kB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]<3,>=2.47->tfx) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]<3,>=2.47->tfx) (3.4)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tfx) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tfx) (3.5.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tfx) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tfx) (3.0.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib->kubernetes<13,>=10.0.1->tfx) (3.2.2)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (6.1.12)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (6.3.2)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (0.8.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (0.2.9)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (6.5.5)\n",
            "Requirement already satisfied: pyzmq<25,>=17 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (23.2.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (23.1.0)\n",
            "Requirement already satisfied: jupyter-core>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (5.5.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (5.9.2)\n",
            "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (6.5.4)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (1.5.8)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (1.8.2)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (0.17.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (0.18.0)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (1.0.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.6.1->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (3.11.0)\n",
            "Requirement already satisfied: jupyter-server>=1.8 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (1.24.0)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (0.2.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (4.9.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (4.11.2)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (6.1.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (0.7.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (0.4)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (0.2.2)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (0.8.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (0.8.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (1.5.0)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (1.2.1)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (2.18.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (4.19.2)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (21.2.0)\n",
            "INFO: pip is looking at multiple versions of jsonschema to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting jsonschema>=2.6 (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx)\n",
            "  Downloading jsonschema-4.19.1-py3-none-any.whl (83 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.3/83.3 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading jsonschema-4.19.0-py3-none-any.whl (83 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.4/83.4 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading jsonschema-4.18.6-py3-none-any.whl (83 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.2/83.2 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading jsonschema-4.18.5-py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.5/82.5 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading jsonschema-4.18.4-py3-none-any.whl (80 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.0/81.0 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading jsonschema-4.18.3-py3-none-any.whl (80 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.0/81.0 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading jsonschema-4.18.2-py3-none-any.whl (80 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.8/80.8 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: pip is looking at multiple versions of jsonschema to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading jsonschema-4.18.1-py3-none-any.whl (80 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.7/80.7 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading jsonschema-4.18.0-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.5/81.5 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading jsonschema-4.17.3-py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.4/90.4 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx)\n",
            "  Downloading pyrsistent-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.7/117.7 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (3.7.1)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (1.16.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (2.5)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (0.5.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (1.1.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (2.21)\n",
            "Building wheels for collected packages: google-apitools, crcmod, dill, hdfs, pyfarmhash, docopt\n",
            "  Building wheel for google-apitools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for google-apitools: filename=google_apitools-0.5.31-py3-none-any.whl size=131016 sha256=0f4c0fc41f95c52cb6ae692a962a31e65a9d64267dd5681c6b63e73827f198b7\n",
            "  Stored in directory: /root/.cache/pip/wheels/04/b7/e0/9712f8c23a5da3d9d16fb88216b897bf60e85b12f5470f26ee\n",
            "  Building wheel for crcmod (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for crcmod: filename=crcmod-1.7-cp310-cp310-linux_x86_64.whl size=31409 sha256=b89c9118ff07e8420c31009115c99dec89fabfce229bbdda4ce2e1d78c7e4051\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/4c/07/72215c529bd59d67e3dac29711d7aba1b692f543c808ba9e86\n",
            "  Building wheel for dill (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dill: filename=dill-0.3.1.1-py3-none-any.whl size=78542 sha256=1b536dd19345e7c32bc26f609321cbf379257ea8d371613cf6aaebd7b3682e24\n",
            "  Stored in directory: /root/.cache/pip/wheels/ea/e2/86/64980d90e297e7bf2ce588c2b96e818f5399c515c4bb8a7e4f\n",
            "  Building wheel for hdfs (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hdfs: filename=hdfs-2.7.3-py3-none-any.whl size=34325 sha256=d51d8bc3a30c622adfd2cda83cdcaf2bae347ce175aa9b6b5cb8c6ae41f211e2\n",
            "  Stored in directory: /root/.cache/pip/wheels/e5/8d/b6/99c1c0a3ac5788c866b0ecd3f48b0134a5910e6ed26011800b\n",
            "  Building wheel for pyfarmhash (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyfarmhash: filename=pyfarmhash-0.3.2-cp310-cp310-linux_x86_64.whl size=88656 sha256=c8f822b4dae0e935ffe4f55cd79363063b76552a520cc59604a100e4915d381a\n",
            "  Stored in directory: /root/.cache/pip/wheels/e0/08/da/f66b1f3258fe3f1e767b2136c5444dbfa9fa3f7944cc5e1983\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=9aa382c35c206126a32c8d5b6917ae673ee7652b0c6ef4fcf11aaad33ed80a1a\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "Successfully built google-apitools crcmod dill hdfs pyfarmhash docopt\n",
            "Installing collected packages: pyfarmhash, kt-legacy, docopt, crcmod, zstandard, uritemplate, tensorflow-hub, tensorflow-estimator, pyrsistent, pyarrow, packaging, overrides, orjson, objsize, keras, jedi, gast, fasteners, fastavro, dnspython, dill, attrs, pymongo, ml-metadata, keras-tuner, jsonschema, hdfs, docker, kubernetes, google-apitools, apache-beam, tensorboard, google-api-python-client, tensorflow, ml-pipelines-sdk, google-cloud-vision, google-cloud-videointelligence, google-cloud-spanner, google-cloud-recommendations-ai, google-cloud-pubsub, google-cloud-dlp, google-cloud-bigtable, google-cloud-bigquery, tensorflow-serving-api, google-cloud-pubsublite, google-cloud-aiplatform, tfx-bsl, tensorflow-transform, tensorflow-data-validation, tensorflow-model-analysis, tfx\n",
            "  Attempting uninstall: uritemplate\n",
            "    Found existing installation: uritemplate 4.1.1\n",
            "    Uninstalling uritemplate-4.1.1:\n",
            "      Successfully uninstalled uritemplate-4.1.1\n",
            "  Attempting uninstall: tensorflow-hub\n",
            "    Found existing installation: tensorflow-hub 0.15.0\n",
            "    Uninstalling tensorflow-hub-0.15.0:\n",
            "      Successfully uninstalled tensorflow-hub-0.15.0\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.14.0\n",
            "    Uninstalling tensorflow-estimator-2.14.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.14.0\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 9.0.0\n",
            "    Uninstalling pyarrow-9.0.0:\n",
            "      Successfully uninstalled pyarrow-9.0.0\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 23.2\n",
            "    Uninstalling packaging-23.2:\n",
            "      Successfully uninstalled packaging-23.2\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.14.0\n",
            "    Uninstalling keras-2.14.0:\n",
            "      Successfully uninstalled keras-2.14.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.5.4\n",
            "    Uninstalling gast-0.5.4:\n",
            "      Successfully uninstalled gast-0.5.4\n",
            "  Attempting uninstall: attrs\n",
            "    Found existing installation: attrs 23.1.0\n",
            "    Uninstalling attrs-23.1.0:\n",
            "      Successfully uninstalled attrs-23.1.0\n",
            "  Attempting uninstall: jsonschema\n",
            "    Found existing installation: jsonschema 4.19.2\n",
            "    Uninstalling jsonschema-4.19.2:\n",
            "      Successfully uninstalled jsonschema-4.19.2\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.14.1\n",
            "    Uninstalling tensorboard-2.14.1:\n",
            "      Successfully uninstalled tensorboard-2.14.1\n",
            "  Attempting uninstall: google-api-python-client\n",
            "    Found existing installation: google-api-python-client 2.84.0\n",
            "    Uninstalling google-api-python-client-2.84.0:\n",
            "      Successfully uninstalled google-api-python-client-2.84.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.14.0\n",
            "    Uninstalling tensorflow-2.14.0:\n",
            "      Successfully uninstalled tensorflow-2.14.0\n",
            "  Attempting uninstall: google-cloud-bigquery\n",
            "    Found existing installation: google-cloud-bigquery 3.12.0\n",
            "    Uninstalling google-cloud-bigquery-3.12.0:\n",
            "      Successfully uninstalled google-cloud-bigquery-3.12.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\n",
            "bigframes 0.12.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.10.0, but you have google-cloud-bigquery 2.34.4 which is incompatible.\n",
            "pandas-gbq 0.17.9 requires pyarrow<10.0dev,>=3.0.0, but you have pyarrow 10.0.1 which is incompatible.\n",
            "referencing 0.30.2 requires attrs>=22.2.0, but you have attrs 21.4.0 which is incompatible.\n",
            "statsmodels 0.14.0 requires packaging>=21.3, but you have packaging 20.9 which is incompatible.\n",
            "xarray 2023.7.0 requires packaging>=21.3, but you have packaging 20.9 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed apache-beam-2.50.0 attrs-21.4.0 crcmod-1.7 dill-0.3.1.1 dnspython-2.4.2 docker-4.4.4 docopt-0.6.2 fastavro-1.9.0 fasteners-0.19 gast-0.4.0 google-api-python-client-1.12.11 google-apitools-0.5.31 google-cloud-aiplatform-1.36.1 google-cloud-bigquery-2.34.4 google-cloud-bigtable-2.21.0 google-cloud-dlp-3.13.0 google-cloud-pubsub-2.18.4 google-cloud-pubsublite-1.8.3 google-cloud-recommendations-ai-0.10.5 google-cloud-spanner-3.40.1 google-cloud-videointelligence-2.11.4 google-cloud-vision-3.4.5 hdfs-2.7.3 jedi-0.19.1 jsonschema-4.17.3 keras-2.13.1 keras-tuner-1.4.6 kt-legacy-1.0.5 kubernetes-12.0.1 ml-metadata-1.14.0 ml-pipelines-sdk-1.14.0 objsize-0.6.1 orjson-3.9.10 overrides-6.5.0 packaging-20.9 pyarrow-10.0.1 pyfarmhash-0.3.2 pymongo-4.6.0 pyrsistent-0.20.0 tensorboard-2.13.0 tensorflow-2.13.1 tensorflow-data-validation-1.14.0 tensorflow-estimator-2.13.0 tensorflow-hub-0.13.0 tensorflow-model-analysis-0.45.0 tensorflow-serving-api-2.13.1 tensorflow-transform-1.14.0 tfx-1.14.0 tfx-bsl-1.14.0 uritemplate-3.0.1 zstandard-0.22.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google",
                  "packaging"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvdjUEUJEuKW"
      },
      "source": [
        "Did you restart the runtime?\n",
        "If you are using Google Colab, the first time that you run the cell above, you must restart the runtime by clicking above \"RESTART RUNTIME\" button or using \"Runtime > Restart runtime ...\" menu. This is because of the way that Colab loads packages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "5SVw9roJ2Csz"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tfx.components import CsvExampleGen\n",
        "from tfx.components import ExampleValidator\n",
        "from tfx.components import SchemaGen\n",
        "from tfx.components import StatisticsGen\n",
        "from tfx.components import Transform\n",
        "\n",
        "from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext\n",
        "from google.protobuf.json_format import MessageToDict\n",
        "\n",
        "import pandas as pd\n",
        "import pprint\n",
        "pp = pprint.PrettyPrinter()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUkpxFFD2Csz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81648783-7dce-42cd-91f4-d585c2894220"
      },
      "source": [
        "print('TensorFlow version: {}'.format(tf.__version__))\n",
        "import tfx as tfx\n",
        "print('TFX version: {}'.format(tfx.__version__))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.13.1\n",
            "TFX version: 1.14.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mount Google Drive:\n",
        "\n",
        "Execute the following code in a cell of a Google Colab notebook: The cell below contain access to Bestman's drive"
      ],
      "metadata": {
        "id": "u7Y2xkgwAmDw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OFHGkW0xATQ1",
        "outputId": "5cb8eb79-3218-46bb-ba71-74a4d5066914"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AK3_skiv2Cs0"
      },
      "source": [
        "### Define paths\n",
        "\n",
        "You will define a few global variables to indicate paths in the local workspace."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gnNCdFW2Cs1"
      },
      "source": [
        "# # Load the dataset using forward slashes as the Colab runs on Unix Shell\n",
        "\n",
        "\n",
        "link = \"/content/drive/My Drive/Machine Learning Engineering/Bike Sharing Demand Project/SeoulBikeData.csv\"\n",
        "\n",
        "df = pd.read_csv(link, encoding='latin1')\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2cMMAbSkGfX"
      },
      "source": [
        "### Preview the  dataset\n",
        "The data : Seoul Bike Sharing Demand. (2020). UCI Machine Learning Repository. [Seoul Bike](https://doi.org/10.24432/C5F62R).\n",
        "\n",
        "\n",
        "\n",
        "Currently Rental bikes are introduced in many urban cities for the enhancement of mobility comfort. It is important to make the rental bike available and accessible to the public at the right time as it lessens the waiting time. Eventually, providing the city with a stable supply of rental bikes becomes a major concern.\n",
        "##The crucial part is the prediction of bike count required at each hour for the stable supply of rental bikes.\n",
        "The dataset contains weather information (Temperature, Humidity, Windspeed, Visibility, Dewpoint, Solar radiation, Snowfall, Rainfall), the number of bikes rented per hour and date information. The label (target) is the #'Rented Bike count' field.\n",
        "\n",
        "Here is the description of the features again:\n",
        "\n",
        "* **Date** : year-month-day\n",
        "* **Rented Bike count** - Count of bikes rented at each hour\n",
        "* **Hour** - Hour of the day\n",
        "* **Temperature** - Temperature in Celsius\n",
        "* **Humidity** - %\n",
        "* **Windspeed** - m/s\n",
        "* **Visibility** - 10m\n",
        "* **Dew point temperature** - Celsius\n",
        "* **Solar radiation** - MJ/m2\n",
        "* **Rainfall** - mm\n",
        "* **Snowfall** - cm\n",
        "* **Seasons** - Winter, Spring, Summer, Autumn\n",
        "* **Holiday** - Holiday/No holiday\n",
        "* **Functional Day** - NoFunc(Non Functional Hours), Fun(Functional hours)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5YPeLPFOXaD",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 652
        },
        "outputId": "f964c31f-eb4d-4f0c-98a2-13bd9e7029e7"
      },
      "source": [
        "# preview the first few rows of the dataset\n",
        "df"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            Date  Rented Bike Count  Hour  Temperature(°C)  Humidity(%)  \\\n",
              "0     01/12/2017                254     0             -5.2           37   \n",
              "1     01/12/2017                204     1             -5.5           38   \n",
              "2     01/12/2017                173     2             -6.0           39   \n",
              "3     01/12/2017                107     3             -6.2           40   \n",
              "4     01/12/2017                 78     4             -6.0           36   \n",
              "...          ...                ...   ...              ...          ...   \n",
              "8755  30/11/2018               1003    19              4.2           34   \n",
              "8756  30/11/2018                764    20              3.4           37   \n",
              "8757  30/11/2018                694    21              2.6           39   \n",
              "8758  30/11/2018                712    22              2.1           41   \n",
              "8759  30/11/2018                584    23              1.9           43   \n",
              "\n",
              "      Wind speed (m/s)  Visibility (10m)  Dew point temperature(°C)  \\\n",
              "0                  2.2              2000                      -17.6   \n",
              "1                  0.8              2000                      -17.6   \n",
              "2                  1.0              2000                      -17.7   \n",
              "3                  0.9              2000                      -17.6   \n",
              "4                  2.3              2000                      -18.6   \n",
              "...                ...               ...                        ...   \n",
              "8755               2.6              1894                      -10.3   \n",
              "8756               2.3              2000                       -9.9   \n",
              "8757               0.3              1968                       -9.9   \n",
              "8758               1.0              1859                       -9.8   \n",
              "8759               1.3              1909                       -9.3   \n",
              "\n",
              "      Solar Radiation (MJ/m2)  Rainfall(mm)  Snowfall (cm) Seasons  \\\n",
              "0                         0.0           0.0            0.0  Winter   \n",
              "1                         0.0           0.0            0.0  Winter   \n",
              "2                         0.0           0.0            0.0  Winter   \n",
              "3                         0.0           0.0            0.0  Winter   \n",
              "4                         0.0           0.0            0.0  Winter   \n",
              "...                       ...           ...            ...     ...   \n",
              "8755                      0.0           0.0            0.0  Autumn   \n",
              "8756                      0.0           0.0            0.0  Autumn   \n",
              "8757                      0.0           0.0            0.0  Autumn   \n",
              "8758                      0.0           0.0            0.0  Autumn   \n",
              "8759                      0.0           0.0            0.0  Autumn   \n",
              "\n",
              "         Holiday Functioning Day  \n",
              "0     No Holiday             Yes  \n",
              "1     No Holiday             Yes  \n",
              "2     No Holiday             Yes  \n",
              "3     No Holiday             Yes  \n",
              "4     No Holiday             Yes  \n",
              "...          ...             ...  \n",
              "8755  No Holiday             Yes  \n",
              "8756  No Holiday             Yes  \n",
              "8757  No Holiday             Yes  \n",
              "8758  No Holiday             Yes  \n",
              "8759  No Holiday             Yes  \n",
              "\n",
              "[8760 rows x 14 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-305f85f7-00ef-48f8-b5bb-27e31ba33b4e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Rented Bike Count</th>\n",
              "      <th>Hour</th>\n",
              "      <th>Temperature(°C)</th>\n",
              "      <th>Humidity(%)</th>\n",
              "      <th>Wind speed (m/s)</th>\n",
              "      <th>Visibility (10m)</th>\n",
              "      <th>Dew point temperature(°C)</th>\n",
              "      <th>Solar Radiation (MJ/m2)</th>\n",
              "      <th>Rainfall(mm)</th>\n",
              "      <th>Snowfall (cm)</th>\n",
              "      <th>Seasons</th>\n",
              "      <th>Holiday</th>\n",
              "      <th>Functioning Day</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>01/12/2017</td>\n",
              "      <td>254</td>\n",
              "      <td>0</td>\n",
              "      <td>-5.2</td>\n",
              "      <td>37</td>\n",
              "      <td>2.2</td>\n",
              "      <td>2000</td>\n",
              "      <td>-17.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Winter</td>\n",
              "      <td>No Holiday</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>01/12/2017</td>\n",
              "      <td>204</td>\n",
              "      <td>1</td>\n",
              "      <td>-5.5</td>\n",
              "      <td>38</td>\n",
              "      <td>0.8</td>\n",
              "      <td>2000</td>\n",
              "      <td>-17.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Winter</td>\n",
              "      <td>No Holiday</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>01/12/2017</td>\n",
              "      <td>173</td>\n",
              "      <td>2</td>\n",
              "      <td>-6.0</td>\n",
              "      <td>39</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2000</td>\n",
              "      <td>-17.7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Winter</td>\n",
              "      <td>No Holiday</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>01/12/2017</td>\n",
              "      <td>107</td>\n",
              "      <td>3</td>\n",
              "      <td>-6.2</td>\n",
              "      <td>40</td>\n",
              "      <td>0.9</td>\n",
              "      <td>2000</td>\n",
              "      <td>-17.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Winter</td>\n",
              "      <td>No Holiday</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>01/12/2017</td>\n",
              "      <td>78</td>\n",
              "      <td>4</td>\n",
              "      <td>-6.0</td>\n",
              "      <td>36</td>\n",
              "      <td>2.3</td>\n",
              "      <td>2000</td>\n",
              "      <td>-18.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Winter</td>\n",
              "      <td>No Holiday</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8755</th>\n",
              "      <td>30/11/2018</td>\n",
              "      <td>1003</td>\n",
              "      <td>19</td>\n",
              "      <td>4.2</td>\n",
              "      <td>34</td>\n",
              "      <td>2.6</td>\n",
              "      <td>1894</td>\n",
              "      <td>-10.3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Autumn</td>\n",
              "      <td>No Holiday</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8756</th>\n",
              "      <td>30/11/2018</td>\n",
              "      <td>764</td>\n",
              "      <td>20</td>\n",
              "      <td>3.4</td>\n",
              "      <td>37</td>\n",
              "      <td>2.3</td>\n",
              "      <td>2000</td>\n",
              "      <td>-9.9</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Autumn</td>\n",
              "      <td>No Holiday</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8757</th>\n",
              "      <td>30/11/2018</td>\n",
              "      <td>694</td>\n",
              "      <td>21</td>\n",
              "      <td>2.6</td>\n",
              "      <td>39</td>\n",
              "      <td>0.3</td>\n",
              "      <td>1968</td>\n",
              "      <td>-9.9</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Autumn</td>\n",
              "      <td>No Holiday</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8758</th>\n",
              "      <td>30/11/2018</td>\n",
              "      <td>712</td>\n",
              "      <td>22</td>\n",
              "      <td>2.1</td>\n",
              "      <td>41</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1859</td>\n",
              "      <td>-9.8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Autumn</td>\n",
              "      <td>No Holiday</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8759</th>\n",
              "      <td>30/11/2018</td>\n",
              "      <td>584</td>\n",
              "      <td>23</td>\n",
              "      <td>1.9</td>\n",
              "      <td>43</td>\n",
              "      <td>1.3</td>\n",
              "      <td>1909</td>\n",
              "      <td>-9.3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Autumn</td>\n",
              "      <td>No Holiday</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8760 rows × 14 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-305f85f7-00ef-48f8-b5bb-27e31ba33b4e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-305f85f7-00ef-48f8-b5bb-27e31ba33b4e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-305f85f7-00ef-48f8-b5bb-27e31ba33b4e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-c3d7735e-a5f2-418e-b234-175daa6ba369\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c3d7735e-a5f2-418e-b234-175daa6ba369')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-c3d7735e-a5f2-418e-b234-175daa6ba369 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "CayFbwny2Cs5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cc09354-ce61-4a4d-896d-063c873305b7"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 8760 entries, 0 to 8759\n",
            "Data columns (total 14 columns):\n",
            " #   Column                     Non-Null Count  Dtype  \n",
            "---  ------                     --------------  -----  \n",
            " 0   Date                       8760 non-null   object \n",
            " 1   Rented Bike Count          8760 non-null   int64  \n",
            " 2   Hour                       8760 non-null   int64  \n",
            " 3   Temperature(°C)            8760 non-null   float64\n",
            " 4   Humidity(%)                8760 non-null   int64  \n",
            " 5   Wind speed (m/s)           8760 non-null   float64\n",
            " 6   Visibility (10m)           8760 non-null   int64  \n",
            " 7   Dew point temperature(°C)  8760 non-null   float64\n",
            " 8   Solar Radiation (MJ/m2)    8760 non-null   float64\n",
            " 9   Rainfall(mm)               8760 non-null   float64\n",
            " 10  Snowfall (cm)              8760 non-null   float64\n",
            " 11  Seasons                    8760 non-null   object \n",
            " 12  Holiday                    8760 non-null   object \n",
            " 13  Functioning Day            8760 non-null   object \n",
            "dtypes: float64(6), int64(4), object(4)\n",
            "memory usage: 958.2+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "id": "SqUGTIWJG5cm",
        "outputId": "11c4d07d-68c8-4fad-c245-07023248f500"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       Rented Bike Count         Hour  Temperature(°C)  Humidity(%)  \\\n",
              "count        8760.000000  8760.000000      8760.000000  8760.000000   \n",
              "mean          704.602055    11.500000        12.882922    58.226256   \n",
              "std           644.997468     6.922582        11.944825    20.362413   \n",
              "min             0.000000     0.000000       -17.800000     0.000000   \n",
              "25%           191.000000     5.750000         3.500000    42.000000   \n",
              "50%           504.500000    11.500000        13.700000    57.000000   \n",
              "75%          1065.250000    17.250000        22.500000    74.000000   \n",
              "max          3556.000000    23.000000        39.400000    98.000000   \n",
              "\n",
              "       Wind speed (m/s)  Visibility (10m)  Dew point temperature(°C)  \\\n",
              "count       8760.000000       8760.000000                8760.000000   \n",
              "mean           1.724909       1436.825799                   4.073813   \n",
              "std            1.036300        608.298712                  13.060369   \n",
              "min            0.000000         27.000000                 -30.600000   \n",
              "25%            0.900000        940.000000                  -4.700000   \n",
              "50%            1.500000       1698.000000                   5.100000   \n",
              "75%            2.300000       2000.000000                  14.800000   \n",
              "max            7.400000       2000.000000                  27.200000   \n",
              "\n",
              "       Solar Radiation (MJ/m2)  Rainfall(mm)  Snowfall (cm)  \n",
              "count              8760.000000   8760.000000    8760.000000  \n",
              "mean                  0.569111      0.148687       0.075068  \n",
              "std                   0.868746      1.128193       0.436746  \n",
              "min                   0.000000      0.000000       0.000000  \n",
              "25%                   0.000000      0.000000       0.000000  \n",
              "50%                   0.010000      0.000000       0.000000  \n",
              "75%                   0.930000      0.000000       0.000000  \n",
              "max                   3.520000     35.000000       8.800000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c67fed43-b986-49a9-bdd2-4640778fd197\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Rented Bike Count</th>\n",
              "      <th>Hour</th>\n",
              "      <th>Temperature(°C)</th>\n",
              "      <th>Humidity(%)</th>\n",
              "      <th>Wind speed (m/s)</th>\n",
              "      <th>Visibility (10m)</th>\n",
              "      <th>Dew point temperature(°C)</th>\n",
              "      <th>Solar Radiation (MJ/m2)</th>\n",
              "      <th>Rainfall(mm)</th>\n",
              "      <th>Snowfall (cm)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>8760.000000</td>\n",
              "      <td>8760.000000</td>\n",
              "      <td>8760.000000</td>\n",
              "      <td>8760.000000</td>\n",
              "      <td>8760.000000</td>\n",
              "      <td>8760.000000</td>\n",
              "      <td>8760.000000</td>\n",
              "      <td>8760.000000</td>\n",
              "      <td>8760.000000</td>\n",
              "      <td>8760.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>704.602055</td>\n",
              "      <td>11.500000</td>\n",
              "      <td>12.882922</td>\n",
              "      <td>58.226256</td>\n",
              "      <td>1.724909</td>\n",
              "      <td>1436.825799</td>\n",
              "      <td>4.073813</td>\n",
              "      <td>0.569111</td>\n",
              "      <td>0.148687</td>\n",
              "      <td>0.075068</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>644.997468</td>\n",
              "      <td>6.922582</td>\n",
              "      <td>11.944825</td>\n",
              "      <td>20.362413</td>\n",
              "      <td>1.036300</td>\n",
              "      <td>608.298712</td>\n",
              "      <td>13.060369</td>\n",
              "      <td>0.868746</td>\n",
              "      <td>1.128193</td>\n",
              "      <td>0.436746</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-17.800000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>27.000000</td>\n",
              "      <td>-30.600000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>191.000000</td>\n",
              "      <td>5.750000</td>\n",
              "      <td>3.500000</td>\n",
              "      <td>42.000000</td>\n",
              "      <td>0.900000</td>\n",
              "      <td>940.000000</td>\n",
              "      <td>-4.700000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>504.500000</td>\n",
              "      <td>11.500000</td>\n",
              "      <td>13.700000</td>\n",
              "      <td>57.000000</td>\n",
              "      <td>1.500000</td>\n",
              "      <td>1698.000000</td>\n",
              "      <td>5.100000</td>\n",
              "      <td>0.010000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1065.250000</td>\n",
              "      <td>17.250000</td>\n",
              "      <td>22.500000</td>\n",
              "      <td>74.000000</td>\n",
              "      <td>2.300000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>14.800000</td>\n",
              "      <td>0.930000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>3556.000000</td>\n",
              "      <td>23.000000</td>\n",
              "      <td>39.400000</td>\n",
              "      <td>98.000000</td>\n",
              "      <td>7.400000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>27.200000</td>\n",
              "      <td>3.520000</td>\n",
              "      <td>35.000000</td>\n",
              "      <td>8.800000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c67fed43-b986-49a9-bdd2-4640778fd197')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c67fed43-b986-49a9-bdd2-4640778fd197 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c67fed43-b986-49a9-bdd2-4640778fd197');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-a5126f5f-3250-47d7-8406-7f87d4b4505d\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a5126f5f-3250-47d7-8406-7f87d4b4505d')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-a5126f5f-3250-47d7-8406-7f87d4b4505d button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0MUk5tm2Cs6"
      },
      "source": [
        "# location of the pipeline metadata store\n",
        "_pipeline_root = './pipeline/'\n",
        "\n",
        "# directory of the raw data files\n",
        "_data_root = './bikedata'\n",
        "\n",
        "if not os.path.exists(_data_root):\n",
        "    os.makedirs(_data_root)\n",
        "\n",
        "# path to the raw training data\n",
        "_data_filepath = os.path.join(_data_root, 'SeoulBikeData.csv')\n",
        "\n",
        "# save the data to a .csv file\n",
        "df.to_csv(_data_filepath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ONIE_hdkPS4"
      },
      "source": [
        "### Create the Interactive Context\n",
        "\n",
        "When pushing to production, you want to automate the pipeline execution using orchestrators such as [Apache Beam](https://beam.apache.org/) and [Kubeflow](https://www.kubeflow.org/). You will not be doing that just yet and will instead execute the pipeline from this notebook. When experimenting in a notebook environment, you will be *manually* executing the pipeline components (i.e. you are the orchestrator). For that, TFX provides the [Interactive Context](https://www.tensorflow.org/tfx/api_docs/python/tfx/orchestration/experimental/interactive/interactive_context/InteractiveContext) so you can step through each component and inspect its outputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3q3VtEW2Cs7"
      },
      "source": [
        "You will initialize the `InteractiveContext` below. This will create a database in the `_pipeline_root` directory which the different components will use to save or get the state of the component executions. You will learn more about this in Week 3 when we discuss ML Metadata. For now, you can think of it as the data store that makes it possible for the different pipeline components to work together.\n",
        "\n",
        "*Note: You can configure the database to connect to but for this exercise, we will just use the default which is a newly created local sqlite file.* ***You will see the warning after running the cell below and you can safely ignore it.***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Rh6K5sUf9dd",
        "collapsed": true
      },
      "source": [
        "# Initialize the InteractiveContext with a local sqlite file.\n",
        "# If you leave `_pipeline_root` blank, then the db will be created in a temporary directory.\n",
        "# You can safely ignore the warning about the missing config file.\n",
        "context = InteractiveContext(pipeline_root=_pipeline_root)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdQWxfsVkzdJ"
      },
      "source": [
        "## Run TFX components interactively\n",
        "\n",
        "With that, you can now run the pipeline interactively. You will see how to do that as you go through the different components below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9fwt9gQk3BR"
      },
      "source": [
        "### ExampleGen\n",
        "\n",
        "You will start the pipeline with the [ExampleGen](https://www.tensorflow.org/tfx/guide/examplegen) component. This  will:\n",
        "\n",
        "*   split the data into training and evaluation sets (by default: 2/3 train, 1/3 eval).\n",
        "*   convert each data row into `tf.train.Example` format. This [protocol buffer](https://developers.google.com/protocol-buffers) is designed for Tensorflow operations and is used by the TFX components.\n",
        "*   compress and save the data collection under the `_pipeline_root` directory for other components to access. These examples are stored in `TFRecord` format. This optimizes read and write operations within Tensorflow especially if you have a large collection of data.\n",
        "\n",
        "Its constructor takes the path to your data source/directory. In our case, this is the `_data_root` path. The component supports several data sources such as CSV, tf.Record, and BigQuery. Since our data is a CSV file, we will use [CsvExampleGen](https://www.tensorflow.org/tfx/api_docs/python/tfx/components/CsvExampleGen) to ingest the data.\n",
        "\n",
        "Run the cell below to instantiate `CsvExampleGen`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyXjuMt8f-9u",
        "scrolled": true
      },
      "source": [
        "# Instantiate ExampleGen with the input CSV dataset\n",
        "example_gen = CsvExampleGen(input_base=_data_root)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8u5Cy94Z2Cs9"
      },
      "source": [
        "You can execute the component by calling the `run()` method of the `InteractiveContext`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "O7gegkP02Cs-"
      },
      "source": [
        "# Execute the component\n",
        "context.run(example_gen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqCoZh7KPUm9"
      },
      "source": [
        "You will notice that an output cell showing the execution results is automatically shown. This metadata is recorded into the database created earlier. This allows you to keep track of your project runs. For example, if you run it again, you will notice the `.execution_id` incrementing.\n",
        "\n",
        "The output of the components are called *artifacts* and you can see an example by navigating through  `.component.outputs > ['examples'] > Channel > ._artifacts > [0]` above. It shows information such as where the converted data is stored (`.uri`) and the splits generated (`.split_names`).\n",
        "\n",
        "You can also examine the output artifacts programmatically with the code below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "880KkTAkPeUg"
      },
      "source": [
        "# get the artifact object\n",
        "artifact = example_gen.outputs['examples'].get()[0]\n",
        "\n",
        "# print split names and uri\n",
        "print(f'split names: {artifact.split_names}')\n",
        "print(f'artifact uri: {artifact.uri}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uz14UOcs2Cs_"
      },
      "source": [
        "If you're wondering , the `number` in `./pipeline/CsvExampleGen/examples/{number}` is the execution id associated with that dataset. If you restart the kernel of this workspace and re-run up to this cell, you will notice a new folder with a different id name created. This shows that TFX is keeping versions of your data so you can roll back if you want to investigate a particular execution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6vcbW_wPqvl"
      },
      "source": [
        "As mentioned, the ingested data is stored in the directory shown in the `uri` field. It is also compressed using `gzip` and you can verify by running the cell below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcZUOlt12CtA"
      },
      "source": [
        "# Get the URI of the output artifact representing the training examples\n",
        "train_uri = os.path.join(artifact.uri, 'Split-train')\n",
        "\n",
        "# See the contents of the `train` folder\n",
        "!ls {train_uri}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mm8dEmpH2CtA"
      },
      "source": [
        "In a notebook environment, it may be useful to examine a few examples of the data especially if you're still experimenting. Since the data collection is saved in [TFRecord format](https://www.tensorflow.org/tutorials/load_data/tfrecord), you will need to use methods that work with that data type. You will need to unpack the individual examples from the `TFRecord` file and format it for printing. Let's do that in the following cells:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4XIXjiCPwzQ"
      },
      "source": [
        "# Get the list of files in this directory (all compressed TFRecord files)\n",
        "tfrecord_filenames = [os.path.join(train_uri, name)\n",
        "                      for name in os.listdir(train_uri)]\n",
        "\n",
        "print(tfrecord_filenames)\n",
        "\n",
        "# Create a `TFRecordDataset` to read these files\n",
        "dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HC7MODf-2CtB"
      },
      "source": [
        "# Define a helper function to get individual examples\n",
        "def get_records(dataset, num_records):\n",
        "    '''Extracts records from the given dataset.\n",
        "    Args:\n",
        "        dataset (TFRecordDataset): dataset saved by ExampleGen\n",
        "        num_records (int): number of records to preview\n",
        "    '''\n",
        "\n",
        "    # initialize an empty list\n",
        "    records = []\n",
        "\n",
        "    # Use the `take()` method to specify how many records to get\n",
        "    for tfrecord in dataset.take(num_records):\n",
        "\n",
        "        # Get the numpy property of the tensor\n",
        "        serialized_example = tfrecord.numpy()\n",
        "\n",
        "        # Initialize a `tf.train.Example()` to read the serialized data\n",
        "        example = tf.train.Example()\n",
        "\n",
        "        # Read the example data (output is a protocol buffer message)\n",
        "        example.ParseFromString(serialized_example)\n",
        "\n",
        "        # convert the protocol bufffer message to a Python dictionary\n",
        "        example_dict = (MessageToDict(example))\n",
        "\n",
        "        # append to the records list\n",
        "        records.append(example_dict)\n",
        "\n",
        "    return records"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUirzkIQ2CtC"
      },
      "source": [
        "# Get 3 records from the dataset\n",
        "sample_records = get_records(dataset, 3)\n",
        "\n",
        "print(type(sample_records[0]))\n",
        "\n",
        "# Print the output\n",
        "pp.pprint(sample_records)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gluYjccf-IP"
      },
      "source": [
        "Now that `ExampleGen` has finished ingesting the data, the next step is data analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csM6BFhtk5Aa"
      },
      "source": [
        "### StatisticsGen\n",
        "The [StatisticsGen](https://www.tensorflow.org/tfx/guide/statsgen) component computes statistics over your dataset for data analysis, as well as for use in downstream components (i.e. next steps in the pipeline). As mentioned earlier, this component uses TFDV under the hood so its output will be familiar to you.\n",
        "\n",
        "`StatisticsGen` takes as input the dataset we just ingested using `CsvExampleGen`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAscCCYWgA-9"
      },
      "source": [
        "# Instantiate StatisticsGen with the ExampleGen ingested dataset\n",
        "statistics_gen = StatisticsGen(\n",
        "    examples=example_gen.outputs['examples'])\n",
        "\n",
        "# Execute the component\n",
        "context.run(statistics_gen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbx5p3ow2CtD"
      },
      "source": [
        "You can display the statistics with the `show()` method.\n",
        "\n",
        "*Note: You can safely ignore the warning shown when running the cell below.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1T05Mo0G2CtD"
      },
      "source": [
        "# Show the output statistics\n",
        "context.show(statistics_gen.outputs['statistics'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLKLTO9Nk60p"
      },
      "source": [
        "### SchemaGen\n",
        "\n",
        "The [SchemaGen](https://www.tensorflow.org/tfx/guide/schemagen) component also uses TFDV to generate a schema based on your data statistics. As you've learned previously, a schema defines the expected bounds, types, and properties of the features in your dataset.\n",
        "\n",
        "`SchemaGen` will take as input the statistics that we generated with `StatisticsGen`, looking at the training split by default."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygQvZ6hsiQ_J"
      },
      "source": [
        "# Instantiate SchemaGen with the StatisticsGen ingested dataset\n",
        "schema_gen = SchemaGen(\n",
        "    statistics=statistics_gen.outputs['statistics'],\n",
        "    )\n",
        "\n",
        "# Run the component\n",
        "context.run(schema_gen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zi6TxTUKXM6b"
      },
      "source": [
        "You can then visualize the generated schema as a table."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ec9vqDXpXeMb"
      },
      "source": [
        "# Visualize the schema\n",
        "context.show(schema_gen.outputs['schema'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXMaQK8E2CtF"
      },
      "source": [
        "Let's now move to the next step in the pipeline and see if there are any anomalies in the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1qcUuO9k9f8"
      },
      "source": [
        "### ExampleValidator\n",
        "\n",
        "The [ExampleValidator](https://www.tensorflow.org/tfx/guide/exampleval) component detects anomalies in your data based on the generated schema from the previous step. Like the previous two components, it also uses TFDV under the hood.\n",
        "\n",
        "`ExampleValidator` will take as input the statistics from `StatisticsGen` and the schema from `SchemaGen`. By default, it compares the statistics from the evaluation split to the schema from the training split."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRlRUuGgiXks"
      },
      "source": [
        "# Instantiate ExampleValidator with the StatisticsGen and SchemaGen ingested data\n",
        "example_validator = ExampleValidator(\n",
        "    statistics=statistics_gen.outputs['statistics'],\n",
        "    schema=schema_gen.outputs['schema'])\n",
        "\n",
        "# Run the component.\n",
        "context.run(example_validator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "855mrHgJcoer"
      },
      "source": [
        "As with the previous component, you can also visualize the anomalies as a table."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDyAAozQcrk3"
      },
      "source": [
        "# Visualize the results\n",
        "context.show(example_validator.outputs['anomalies'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qI0hFUmX2CtG"
      },
      "source": [
        "With no anomalies detected, you can proceed to the next step in the pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPViEz5RlA36"
      },
      "source": [
        "### Transform\n",
        "The [Transform](https://www.tensorflow.org/tfx/guide/transform) component performs feature engineering for both training and serving datasets. It uses the [TensorFlow Transform](https://www.tensorflow.org/tfx/transform/get_started) library introduced in the first ungraded lab of this week.\n",
        "\n",
        "`Transform` will take as input the data from `ExampleGen`, the schema from `SchemaGen`, as well as a module containing the preprocessing function.\n",
        "\n",
        "In this section, you will work on an example of a user-defined Transform code. The pipeline needs to load this as a module so you need to use the magic command `%% writefile` to save the file to disk. Let's first define a few constants that group the data's attributes according to the transforms we will perform later. This file will also be saved locally."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuNSiUKb4YJf"
      },
      "source": [
        "# Set the constants module filename\n",
        "_bikesharing_constants_module_file = 'bikesharing_constants.py'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPjhXuIF4YJh"
      },
      "source": [
        "%%writefile {_bikesharing_constants_module_file}\n",
        "\n",
        "# Features with string data types that will be converted to indices\n",
        "CATEGORICAL_FEATURE_KEYS = [\n",
        "    'Functioning Day', 'Holiday', 'Seasons'\n",
        "]\n",
        "\n",
        "# Numerical features that are marked as continuous\n",
        "NUMERIC_FEATURE_KEYS = ['Humidity(%)', 'Rainfall(mm)', 'Snowfall (cm)', 'Solar Radiation (MJ/m2)', 'Temperature(°C)', 'Visibility (10m)', 'Wind speed (m/s)']\n",
        "\n",
        "# Feature that can be grouped into buckets\n",
        "BUCKET_FEATURE_KEYS = ['Hour']\n",
        "\n",
        "# Number of buckets used by tf.transform for encoding each bucket feature.\n",
        "FEATURE_BUCKET_COUNT = {'Hour': 4}\n",
        "\n",
        "# Feature that the model will predict\n",
        "LABEL_KEY = 'Rented Bike Count'\n",
        "\n",
        "# Utility function for renaming the feature\n",
        "def transformed_name(key):\n",
        "    return key + '_xf'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Duj2Ax5z4YJl"
      },
      "source": [
        "Next, you will work on the module that contains `preprocessing_fn()`. As you've seen in the previous lab, this function defines how you will transform the raw data into features that your model can train on (i.e. the next step in the pipeline). You will use the [tft module functions](https://www.tensorflow.org/tfx/transform/api_docs/python/tft) to make these transformations.\n",
        "\n",
        "*Note: After completing the entire notebook, we encourage you to go back to this section and try different tft functions aside from the ones already provided below. You can also modify the grouping of the feature keys in the constants file if you want. For example, you may want to scale some features to `[0, 1]` while others are scaled to the z-score. This will be good practice for this week's assignment.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AJ9hBs94YJm"
      },
      "source": [
        "# Set the transform module filename\n",
        "_bikesharing_transform_module_file = 'bikesharing_transform.py'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYmxxx9A4YJn"
      },
      "source": [
        "%%writefile {_bikesharing_transform_module_file}\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_transform as tft\n",
        "\n",
        "import bikesharing_constants\n",
        "\n",
        "# Unpack the contents of the constants module\n",
        "_NUMERIC_FEATURE_KEYS = bikesharing_constants.NUMERIC_FEATURE_KEYS\n",
        "_CATEGORICAL_FEATURE_KEYS = bikesharing_constants.CATEGORICAL_FEATURE_KEYS\n",
        "_BUCKET_FEATURE_KEYS = bikesharing_constants.BUCKET_FEATURE_KEYS\n",
        "_FEATURE_BUCKET_COUNT = bikesharing_constants.FEATURE_BUCKET_COUNT\n",
        "_LABEL_KEY = bikesharing_constants.LABEL_KEY\n",
        "_transformed_name = bikesharing_constants.transformed_name\n",
        "\n",
        "\n",
        "# Define the transformations\n",
        "def preprocessing_fn(inputs):\n",
        "    \"\"\"tf.transform's callback function for preprocessing inputs.\n",
        "    Args:\n",
        "        inputs: map from feature keys to raw not-yet-transformed features.\n",
        "    Returns:\n",
        "        Map from string feature key to transformed feature operations.\n",
        "    \"\"\"\n",
        "    outputs = {}\n",
        "\n",
        "    # Scale these features to the range [0,1]\n",
        "    for key in _NUMERIC_FEATURE_KEYS:\n",
        "        outputs[_transformed_name(key)] = tft.scale_to_0_1(inputs[key])\n",
        "\n",
        "    # Bucketize these features\n",
        "    for key in _BUCKET_FEATURE_KEYS:\n",
        "        outputs[_transformed_name(key)] = tft.bucketize(\n",
        "            inputs[key], _FEATURE_BUCKET_COUNT[key])\n",
        "\n",
        "    # Convert strings to indices in a vocabulary\n",
        "    for key in _CATEGORICAL_FEATURE_KEYS:\n",
        "        outputs[_transformed_name(key)] = tft.compute_and_apply_vocabulary(inputs[key])\n",
        "\n",
        "    # Convert the label strings to an index\n",
        "    outputs[_transformed_name(_LABEL_KEY)] = tft.scale_to_0_1(inputs[_LABEL_KEY])\n",
        "\n",
        "    return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgbmZr3sgbWW"
      },
      "source": [
        "You can now pass the training data, schema, and transform module to the `Transform` component. You can ignore the warning messages generated by Apache Beam regarding type hints."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHfhth_GiZI9"
      },
      "source": [
        "# Ignore TF warning messages\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "\n",
        "# Instantiate the Transform component\n",
        "transform = Transform(\n",
        "    examples=example_gen.outputs['examples'],\n",
        "    schema=schema_gen.outputs['schema'],\n",
        "    module_file=os.path.abspath(_bikesharing_transform_module_file))\n",
        "\n",
        "# Run the component\n",
        "context.run(transform)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwAwb4rARRQ2"
      },
      "source": [
        "Let's examine the output artifacts of `Transform` (i.e. `.component.outputs` from the output cell above). This component produces several outputs:\n",
        "\n",
        "* `transform_graph` is the graph that can perform the preprocessing operations. This graph will be included during training and serving to ensure consistent transformations of incoming data.\n",
        "* `transformed_examples` points to the preprocessed training and evaluation data.\n",
        "* `updated_analyzer_cache` are stored calculations from previous runs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyFkBd9AR1sy"
      },
      "source": [
        "Take a peek at the `transform_graph` artifact.  It points to a directory containing three subdirectories."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tRw4DneR3i7"
      },
      "source": [
        "# Get the uri of the transform graph\n",
        "transform_graph_uri = transform.outputs['transform_graph'].get()[0].uri\n",
        "\n",
        "# List the subdirectories under the uri\n",
        "os.listdir(transform_graph_uri)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fqV54CIR6Pu"
      },
      "source": [
        "* The `metadata` subdirectory contains the schema of the original data.\n",
        "* The `transformed_metadata` subdirectory contains the schema of the preprocessed data.\n",
        "* The `transform_fn` subdirectory contains the actual preprocessing graph.\n",
        "\n",
        "You can also take a look at the first three transformed examples using the helper function defined earlier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwbW2zPKR_S4"
      },
      "source": [
        "# Get the URI of the output artifact representing the transformed examples\n",
        "train_uri = os.path.join(transform.outputs['transformed_examples'].get()[0].uri, 'Split-train')\n",
        "\n",
        "# Get the list of files in this directory (all compressed TFRecord files)\n",
        "tfrecord_filenames = [os.path.join(train_uri, name)\n",
        "                      for name in os.listdir(train_uri)]\n",
        "\n",
        "# Create a `TFRecordDataset` to read these files\n",
        "transformed_dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSDZ2rJC7NQW"
      },
      "source": [
        "# Get 3 records from the dataset\n",
        "sample_records_xf = get_records(transformed_dataset, 3)\n",
        "\n",
        "# Print the output\n",
        "pp.pprint(sample_records_xf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFOlEmdx2CtN"
      },
      "source": [
        "**Congratulations!** You have now executed all the components in our pipeline. You will get hands-on practice as well with training and model evaluation in future courses but for now, we encourage you to try exploring the different components we just discussed. As mentioned earlier, a useful exercise for the upcoming assignment is to be familiar with using different `tft` functions in your transform module."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_uxQG2Hy2CtN"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}